---
title: "Linear Regression"
author: "Sarahy Martinez"
date: "2024-11-12"
output: github_document
---


```{r}
library(tidyverse)
library(p8105.datasets)
set.seed(1)


```

```{r}
data("nyc_airbnb")

#standard data cleaning steps 
nyc_airbnb = 
  nyc_airbnb |> 
  mutate(stars = review_scores_location / 2) |> 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) |> # changed spelling of neighbourhood
  filter(borough != "Staten Island") |> 
  select(price, stars, borough, neighborhood, room_type)
```


## Fit a model 
An good place to start is to consider price as an outcome that may depend on rating and borough. We fit that initial model in the following code.

want to fit the stars (numeric) and borough ( categorical)


```{r}
nyc_airbnb %>% 
  ggplot(aes(x= stars, y =price, color = borough))+ 
  geom_point()
# we see alot of overlap and so what we want to do is fit some line that runs through stars and is different for each borough and will try to get parallel lines. 


```

Lets fit the model we care about 
-> a linear model that relates price to stars and boro. Save it as fit; only fitting one model and later overriding. 
```{r, include=FALSE}

fit = lm(price ~ stars + borough, data = nyc_airbnb)

# look at the results
print(fit)
summary(fit) # get more info with summary
summary(fit)$coef #spits out a look a like table thats  not a df instead a matrix 
# to see if its a df or not 
summary(fit)$coef %>% class
coef(fit)
fitted.values(fit) # record of fitted values 
residuals(fit) # gives us the residuals 
```



The lm function begins with the formula specification – outcome on the left of the ~ and predictors separated by + on the right. As we’ll see shortly, interactions between variables can be specified using *. You can also specify an intercept-only model (outcome ~ 1), a model with no intercept (outcome ~ 0 + ...), and a model using all available predictors (outcome ~ .).

R will treat categorical (factor) covariates appropriately and predictably: indicator variables are created for each non-reference category and included in your model, and the factor level is treated as the reference. As with ggplot, being careful with factors is therefore critical!

```{r}
nyc_airbnb = 
  nyc_airbnb |> 
  mutate(
    borough = fct_infreq(borough),
    room_type = fct_infreq(room_type))

fit = lm(price ~ stars + borough, data = nyc_airbnb)


```

It’s important to note that changing reference categories won’t change “fit” or statistical sigificance, but can affect ease of interpretation.


# Tidying the output 
Lets look at the results better

The output of a lm is an object of class lm – a very specific list that isn’t a dataframe but that can be manipulated using other functions. Some common functions for interacting with lm fits are below, although we omit the output.

The reason that we omit the output is that it’s a huge pain to deal with. summary produces an object of class summary.lm, which is also a list – that’s how we extracted the coefficients using summary(fit)$coef. coef produces a vector of coefficient values, and fitted.values is a vector of fitted values. None of this is tidy.

It’s helpful to know about the products of lm and to know there are a range of ways to interact with models in base R. That said, for the most part it’s easiest to use tidy tools.

The broom package has functions for obtaining a quick summary of the model and for cleaning up the coefficient table.

```{r}
fit |> 
  broom::glance() # gives r squared, adjusted, global test, log likelihood etc. 

broom::glance(fit) # same thing as above

fit |> 
  broom::tidy()  #to fit object and looks like matrix but structured like a data frame

#Both of these functions produce data frames, which makes it straightforward to include the results in subsequent steps.

fit |> 
  broom::tidy() |> 
  select(term, estimate, p.value) |> 
  mutate(term = str_replace(term, "^borough", "Borough: ")) |> # replacing borough with capital Borough
  knitr::kable(digits = 3) #reduced to 3 digits


#implicitly the reference group is the Bronx. 
```

As an aside, broom::tidy works with lots of things, including most of the functions for model fitting you’re likely to run into (survival, mixed models, additive models, …).

# Be in control of factors 
This is because variables are organized in alphabetical order
```{r}
#what if the reference is the most common

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(borough= fct_infreq(borough),
  room_type = fct_infreq(room_type)
  )

```

Look at the plot again 

```{r}

nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price, color = borough))+
  geom_point()

#similar figure but variable order changed in which borough changed from frequency of things
```

```{r}

fit = lm(price ~ stars + borough, data = nyc_airbnb)

broom::tidy(fit)
broom::glance(fit)

# Brooklyn is computed in reference to Manhattan than the Bronx. 
```


# Diagnostics

Regression diagnostics can identify issues in model fit, especially related to certain failures in model assumptions. Examining residuals and fitted values are therefore an important component of any modeling exercise.

The modelr package can be used to add residuals and fitted values to a dataframe.


```{r}

# before we have done 
residuals(fit) # this will then give us a huge vector we don't want 

# Instead take advantage of modelr package 

modelr::add_residuals(nyc_airbnb, fit)  
                    #(dataset, model of interest) then added a column of residuals based on the fit. 


modelr::add_predictions(nyc_airbnb, fit)


#Like many things in the tidyverse, the first argument is a dataframe. That makes it easy to included steps adding residuals or predictions in pipeline of commands to conduct inspections and perform diagnostics.


# Now we want to plot it 
nyc_airbnb |> 
  modelr::add_residuals(fit) |> 
  ggplot(aes(x = borough, y = resid)) + geom_violin()+ #looking at the distribution of residuals in each borough
  ylim(-500,1500) #just zooming in
# in original plot we have larger outlying residuals which we can also see here in the plot 


nyc_airbnb |> 
  modelr::add_residuals(fit) |> 
  ggplot(aes(x = stars, y = resid)) + geom_point()  # looking at the distribution of the stars residuals 
# distribution is telling us that we have some skewness  so we'll see how it plays out in each boro

nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
   ggplot(aes(x = stars, y = resid)) + 
  geom_point()+
  facet_wrap(.~borough)

# we can see that queens has a 5000 dollar residual which is a large outlier. Bronx doesn't have an issue of spreading out. Just an exploratory
```


This example has some obvious issues, most notably the presence of extremely large outliers in price and a generally skewed residual distribution. There are a few things we might try to do here – including creating a formal rule for the exclusion of outliers, transforming the price variable (e.g. using a log transformation), or fitting a model that is robust to outliers. Dealing with these issues isn’t really the purpose of this class, though, so we’ll note the issues and move on; shortly we’ll look at using the bootstrap for inference in cases like this, where standard approaches to inference may fail.

(For what it’s worth, I’d probably use a combination of median regression, which is less sensitive to outliers than OLS, and maybe bootstrapping for inference. If that’s not feasible, I’d omit rentals with price over $1000 (< 0.5% of the sample) from the primary analysis and examine these separately. I usually avoid transforming the outcome, because the results model is difficult to interpret.)


# Hypothesis testing
We’ll comment briefly on hypothesis testing. Model summaries include results of t-tests for single coefficients, and are the standard way of assessing statistical significance.

Testing multiple coefficients is somewhat more complicated. A useful approach is to use nested models, meaning that the terms in a simple “null” model are a subset of the terms in a more complex “alternative” model. The are formal tests for comparing the null and alternative models, even when several coefficients are added in the alternative model. Tests of this kind are required to assess the significance of a categorical predictor with more than two levels, as in the example below.

```{r}
fit %>% 
  broom::tidy()
# we are doing a hypothesis test in the stars estimate, st error with p-value and a t test it's also doing it for the boroughs. If we have a cat predictor with different levels, we don't want to ask if individual levels differ from the reference. Instead we wanna ask if the categorical variable as a whole is significant


```

What about significance of borough? We have to do F test and Anova 
- so we must fit_null ( a null model) price against stars if the null is that borough is not associated with price is true and we won't place in model. 

- Alt model is price against stars plus borough 
```{r}
fit_null = lm(price ~ stars + borough, data = nyc_airbnb) #no borough
fit_alt = lm(price ~ stars + borough + room_type, data = nyc_airbnb) #with borough

# The test of interest is implemented in the anova() function which, of course, can be summarized using broom::tidy().

anova(fit_null, fit_alt) |>   # anova is fitting null vs alt 
  broom::tidy()  # tidying up a bit where we can look at statistic and p-value 


```

Note that this works for nested models only. Comparing non-nested models is a common problem that requires other methods; we’ll see one approach in cross validation.

# Nesting data AND fitting models 
We’ll now turn our attention to fitting models to datasets nested within variables – meaning, essentially, that we’ll use nest() to create a list column containing datasets and fit separate models to each. This is very different from fitting nested models, even though the terminology is similar.

In the airbnb data, we might think that star ratings and room type affects price differently in each borough. One way to allow this kind of effect modification is through interaction terms:

* Is there an association with stars and price and does it differ by borough?

```{r}
nyc_airbnb |> 
  lm(price ~ stars * borough + room_type * borough, data = _) |> #interaction terms 
  broom::tidy() |> 
  knitr::kable(digits = 3)


```

This works, but the output takes time to think through – the expected change in price comparing an entire apartment to a private room in Queens, for example, involves the main effect of room type and the Queens / private room interaction.

Alternatively, we can nest within boroughs and fit borough-specific models associating price with rating and room type:


```{r}

# we want to nest everything except for boro
nest_lm_res =
  nyc_airbnb |>  
  nest(data = -borough) |>  # we have a tibble for boro and an entire df. Given this in a list column we can ask if its possible to fit a model to each variable by using mutate and map. 
  mutate(
    models = map(data, \(df) lm(price ~ stars + room_type, data = df)),
    results = map(models, broom::tidy)) |> 
  select(-data, -models) |> 
  unnest(results)

# other method
 
nyc_airbnb %>% 
  nest(data = - borough) %>% 
  mutate(
    models = map(.x = data, ~lm(price~stars, data = .x)), #fitting a model across. broom tidying is tidying across models and creates a list column
    results = map(models, broom::tidy)
  ) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  filter(term == "stars") # exploratory

# another way of tidying
nyc_airbnb %>% 
  nest(data = - borough) %>% 
  mutate(
    models = map(.x = data, ~lm(price~stars, data = .x)), #fitting a model across. broom tidying is tidying across models and creates a list column
    results = map(models, broom::tidy)
  ) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  filter(term != "(Intercept") %>% 
  select(borough, term, estimate) %>% 
  pivot_wider(
    names_from = borough,
    values_from = estimate
  )

# nesiting data and lookin at results is helful but lest nest even more if we want room type and stars in context of neighborhood within manhattan.

nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  nest(data = - neighborhood) %>% 
  mutate(
    models = map(.x = data, ~lm(price~ stars + room_type, data = .x)), #nesting using this df and others then tidy others 
    results = map(models, broom::tidy)
  ) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% #unnest bc we dont need the df. So we are left with the impact of stars. 
  filter(str_detect(term, "room_type")) %>% 
  ggplot(aes(x= neighborhood, y = estimate))+
  geom_point()+
  facet_wrap(. ~ term)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) #adjudting the angle


# from the graph we can see if we are comparing an entire home comapred to a shared room we'll see the areas where you  might save more money. 

```

The estimates here are the same as those in the model containing interactions, but are easier to extract from the output.

Fitting models to nested datasets is a way of performing stratified analyses. These have a tradeoff: stratified models make it easy to interpret covariate effects in each stratum, but don’t provide a mechanism for assessing the significance of differences across strata.

An even more extreme example is the assessment of neighborhood effects in Manhattan. The code chunk below fits neighborhood-specific models:

```{r}
manhattan_airbnb =
  nyc_airbnb |> 
  filter(borough == "Manhattan")

manhattan_nest_lm_res =
  manhattan_airbnb |> 
  nest(data = -neighborhood) |> 
  mutate(
    models = map(data, \(df) lm(price ~ stars + room_type, data = df)),
    results = map(models, broom::tidy)) |> 
  select(-data, -models) |> 
  unnest(results)




```


And the chunk below shows neighborhood-specific estimates for the coefficients related to room type.

```{r}
manhattan_nest_lm_res |> 
  filter(str_detect(term, "room_type")) |> 
  ggplot(aes(x = neighborhood, y = estimate)) + 
  geom_point() + 
  facet_wrap(~term) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
```
There is, generally speaking, a reduction in room price for a private room or a shared room compared to an entire apartment, but this varies quite a bit across neighborhoods.

With this many factor levels, it really isn’t a good idea to fit models with main effects or interactions for each. Instead, you’d be best-off using a mixed model, with random intercepts and slopes for each neighborhood. Although it’s well beyond the scope of this class, code to fit a mixed model with neighborhood-level random intercepts and random slopes for room type is below. And, of course, we can tidy the results using a mixed-model spinoff of the broom package.

```{r, eval=FALSE}
manhattan_airbnb |> 
  lme4::lmer(price ~ stars + room_type + (1 + room_type | neighborhood), data = _) |> 
  broom.mixed::tidy()

```

Mixed models are pretty great!

# Binary outcomes
Linear models are appropriate for outcomes that follow a continuous distribution, but binary outcomes are common. In these cases, logistic regression is a useful analytic framework.

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository; the final CSV is here. You can read their accompanying article here. We’ll use data on unresolved murders in Baltimore, MD to illustrate logistic regression in R. The code below imports, cleans, and generally wrangles the data for analysis.


```{r, eval=FALSE}
baltimore_df = 
  read_csv("data/homicide-data.csv") |> 
  filter(city == "Baltimore") |> 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")) |> 
  select(resolved, victim_age, victim_race, victim_sex)

```

Using these data, we can fit a logistic regression for the binary “resolved” outcome and victim demographics as predictors. This uses the glm function with the family specified to account for the non-Gaussian outcome distribution.

```{r, eval=FALSE}
fit_logistic = 
  baltimore_df |> 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = _, family = binomial()) 
```

Many of the same tools we used to work with lm fits can be used for glm fits. The table below summaries the coefficients from the model fit; because logistic model estimates are log odds ratios, we include a step to compute odds ratios as well.

```{r, eval=FALSE}
fit_logistic |> 
  broom::tidy() |> 
  mutate(OR = exp(estimate)) |>
  select(term, log_OR = estimate, OR, p.value) |> 
  knitr::kable(digits = 3)

```

Homicides in which the victim is Black are substantially less likely to be resolved that those in which the victim is white; for other races the effects are not significant, possible due to small sample sizes. Homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female. The effect of age is statistically significant, but careful data inspections should be conducted before interpreting too deeply.

We can also compute fitted values; similarly to the estimates in the model summary, these are expressed as log odds and can be transformed to produce probabilities for each subject.

```{r, eval=FALSE}
baltimore_df |> 
  modelr::add_predictions(fit_logistic) |> 
  mutate(fitted_prob = boot::inv.logit(pred))

```




























